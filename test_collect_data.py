#!/usr/bin/env python3
"""
æµ‹è¯•collect_data.pyçš„åŠŸèƒ½
ç”¨10ä¸ªè‚¡ç¥¨æ•°æ®æµ‹è¯•æ–­ç‚¹ç»­ä¼ å’Œå¤štokenåŠŸèƒ½
"""

import os
import sys
import time
import sqlite3
import pandas as pd
from loguru import logger

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import config
from collect_data import StockDataCollector, setup_logger, create_sqlite_database, save_to_sqlite

def test_cache_functionality():
    """æµ‹è¯•ç¼“å­˜åŠŸèƒ½"""
    logger.info("å¼€å§‹æµ‹è¯•ç¼“å­˜åŠŸèƒ½...")
    
    # æ£€æŸ¥ç¼“å­˜ç›®å½•æ˜¯å¦å­˜åœ¨
    cache_dir = 'cache'
    if os.path.exists(cache_dir):
        cache_files = os.listdir(cache_dir)
        logger.info(f"ç¼“å­˜ç›®å½•å­˜åœ¨ï¼ŒåŒ…å« {len(cache_files)} ä¸ªæ–‡ä»¶: {cache_files[:5]}...")
    else:
        logger.info("ç¼“å­˜ç›®å½•ä¸å­˜åœ¨ï¼Œå°†åœ¨é¦–æ¬¡è¿è¡Œæ—¶åˆ›å»º")

def test_multiple_tokens():
    """æµ‹è¯•å¤štokené…ç½®"""
    logger.info("å¼€å§‹æµ‹è¯•å¤štokené…ç½®...")
    
    logger.info(f"é…ç½®çš„tokenæ•°é‡: {len(config.TUSHARE_TOKENS)}")
    for i, token in enumerate(config.TUSHARE_TOKENS[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ªçš„éƒ¨åˆ†å†…å®¹
        masked_token = token[:10] + '***' + token[-5:] if len(token) > 15 else '***'
        logger.info(f"Token {i+1}: {masked_token}")

def test_database_functionality():
    """æµ‹è¯•æ•°æ®åº“åŠŸèƒ½"""
    logger.info("å¼€å§‹æµ‹è¯•æ•°æ®åº“åŠŸèƒ½...")
    
    db_path = 'test_stock_analysis.db'
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®åº“
    create_sqlite_database(db_path)
    
    # æ£€æŸ¥æ•°æ®åº“ç»“æ„
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    # è·å–è¡¨åˆ—è¡¨
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
    tables = cursor.fetchall()
    logger.info(f"æ•°æ®åº“è¡¨: {[table[0] for table in tables]}")
    
    # æ£€æŸ¥ç´¢å¼•
    cursor.execute("SELECT name FROM sqlite_master WHERE type='index';")
    indexes = cursor.fetchall()
    logger.info(f"æ•°æ®åº“ç´¢å¼•: {[index[0] for index in indexes]}")
    
    conn.close()
    
    # æ¸…ç†æµ‹è¯•æ•°æ®åº“
    if os.path.exists(db_path):
        os.remove(db_path)
        logger.info(f"å·²æ¸…ç†æµ‹è¯•æ•°æ®åº“: {db_path}")

def test_stock_data_collection():
    """æµ‹è¯•è‚¡ç¥¨æ•°æ®æ”¶é›†åŠŸèƒ½ï¼ˆ10åªè‚¡ç¥¨ï¼‰"""
    logger.info("å¼€å§‹æµ‹è¯•è‚¡ç¥¨æ•°æ®æ”¶é›†åŠŸèƒ½...")
    
    # æ£€æŸ¥tokené…ç½®
    if not config.TUSHARE_TOKENS:
        logger.error("æœªé…ç½®Tushare tokenï¼Œè¯·æ£€æŸ¥.envæ–‡ä»¶")
        return False
    
    # åˆå§‹åŒ–æ”¶é›†å™¨
    collector = StockDataCollector(
        token=config.TUSHARE_TOKENS[0],
        cache_dir='cache',
        batch_size=5,  # å°æ‰¹æ¬¡ä¾¿äºæµ‹è¯•
        use_delay=True
    )
    
    try:
        # è·å–è‚¡ç¥¨åˆ—è¡¨
        logger.info("è·å–è‚¡ç¥¨åˆ—è¡¨...")
        stocks = collector.get_all_stocks()
        if stocks is None or stocks.empty:
            logger.error("è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥")
            return False
        
        # é™åˆ¶ä¸º10åªè‚¡ç¥¨è¿›è¡Œæµ‹è¯•
        test_stocks = stocks.head(10)
        logger.info(f"é€‰æ‹©æµ‹è¯•è‚¡ç¥¨: {list(test_stocks['ts_code'].values)}")
        
        # åˆ†æ‰¹å¤„ç†ï¼ˆæ¯æ‰¹5åªï¼‰
        all_results = []
        batch_size = 5
        total_batches = (len(test_stocks) + batch_size - 1) // batch_size
        
        for i in range(total_batches):
            start_idx = i * batch_size
            end_idx = min((i + 1) * batch_size, len(test_stocks))
            stocks_batch = test_stocks.iloc[start_idx:end_idx]
            
            logger.info(f"å¤„ç†ç¬¬ {i+1}/{total_batches} æ‰¹æ¬¡: {list(stocks_batch['ts_code'].values)}")
            
            # æµ‹è¯•ç¼“å­˜åŠŸèƒ½ï¼ˆç¬¬ä¸€æ¬¡ä¸ä½¿ç”¨ç¼“å­˜ï¼Œç¬¬äºŒæ¬¡ä½¿ç”¨ç¼“å­˜ï¼‰
            use_cache = (i > 0)  # ç¬¬ä¸€æ‰¹ä¸ä½¿ç”¨ç¼“å­˜ï¼Œä¹‹åä½¿ç”¨ç¼“å­˜
            
            batch_data = collector.process_batch(
                stocks_batch,
                start_year=2020,
                end_year=2023,
                use_cache=use_cache
            )
            
            if batch_data:
                logger.info(f"æ‰¹æ¬¡ {i+1} æ”¶é›†åˆ° {len(batch_data)} åªè‚¡ç¥¨çš„æ•°æ®")
                
                # æ£€æŸ¥æ•°æ®ç»“æ„
                for stock_code, stock_info in list(batch_data.items())[:2]:  # åªæ£€æŸ¥å‰2åª
                    data_keys = list(stock_info['data'].keys())
                    logger.info(f"è‚¡ç¥¨ {stock_code} æ•°æ®ç±»å‹: {data_keys}")
                    
                    # æ£€æŸ¥æ¯ä¸ªæ•°æ®ç±»å‹çš„æ•°é‡
                    for key in data_keys:
                        count = len(stock_info['data'][key])
                        logger.info(f"  {key}: {count} æ¡è®°å½•")
                
                all_results.append(batch_data)
            else:
                logger.warning(f"æ‰¹æ¬¡ {i+1} æœªæ”¶é›†åˆ°æ•°æ®")
            
            # æ·»åŠ å»¶æ—¶é¿å…APIé™åˆ¶
            if i < total_batches - 1:
                time.sleep(1)
        
        # ç»Ÿè®¡ç»“æœ
        total_stocks_collected = sum(len(batch) for batch in all_results)
        logger.info(f"æ€»å…±æ”¶é›†åˆ° {total_stocks_collected} åªè‚¡ç¥¨çš„æ•°æ®")
        
        return total_stocks_collected > 0
        
    except Exception as e:
        logger.error(f"è‚¡ç¥¨æ•°æ®æ”¶é›†æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_resume_functionality():
    """æµ‹è¯•æ–­ç‚¹ç»­ä¼ åŠŸèƒ½"""
    logger.info("å¼€å§‹æµ‹è¯•æ–­ç‚¹ç»­ä¼ åŠŸèƒ½...")
    
    cache_dir = 'cache'
    
    # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶
    if os.path.exists(cache_dir):
        cache_files = [f for f in os.listdir(cache_dir) if f.startswith('batch_')]
        if cache_files:
            logger.info(f"å‘ç° {len(cache_files)} ä¸ªç¼“å­˜æ–‡ä»¶")
            
            # é€‰æ‹©ä¸€ä¸ªç¼“å­˜æ–‡ä»¶æµ‹è¯•è¯»å–
            test_cache_file = os.path.join(cache_dir, cache_files[0])
            try:
                import json
                with open(test_cache_file, 'r', encoding='utf-8') as f:
                    cached_data = json.load(f)
                logger.info(f"æˆåŠŸè¯»å–ç¼“å­˜æ–‡ä»¶ {cache_files[0]}, åŒ…å« {len(cached_data)} åªè‚¡ç¥¨")
                
                # æ˜¾ç¤ºç¼“å­˜æ•°æ®ç»“æ„
                if cached_data:
                    first_stock = list(cached_data.keys())[0]
                    data_structure = list(cached_data[first_stock]['data'].keys())
                    logger.info(f"ç¼“å­˜æ•°æ®ç»“æ„: {data_structure}")
                
                return True
            except Exception as e:
                logger.error(f"è¯»å–ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")
                return False
        else:
            logger.info("æœªå‘ç°ç¼“å­˜æ–‡ä»¶ï¼Œéœ€è¦å…ˆè¿è¡Œæ•°æ®æ”¶é›†")
            return False
    else:
        logger.info("ç¼“å­˜ç›®å½•ä¸å­˜åœ¨")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    setup_logger()
    
    logger.info("ğŸš€ å¼€å§‹æµ‹è¯• collect_data.py åŠŸèƒ½")
    logger.info("=" * 60)
    
    # æµ‹è¯•1ï¼šé…ç½®æ£€æŸ¥
    logger.info("ğŸ“‹ æµ‹è¯•1ï¼šæ£€æŸ¥é…ç½®")
    test_multiple_tokens()
    print()
    
    # æµ‹è¯•2ï¼šç¼“å­˜åŠŸèƒ½æ£€æŸ¥
    logger.info("ğŸ’¾ æµ‹è¯•2ï¼šæ£€æŸ¥ç¼“å­˜åŠŸèƒ½")
    test_cache_functionality()
    print()
    
    # æµ‹è¯•3ï¼šæ•°æ®åº“åŠŸèƒ½æµ‹è¯•
    logger.info("ğŸ—„ï¸ æµ‹è¯•3ï¼šæµ‹è¯•æ•°æ®åº“åŠŸèƒ½")
    test_database_functionality()
    print()
    
    # æµ‹è¯•4ï¼šæ–­ç‚¹ç»­ä¼ åŠŸèƒ½æµ‹è¯•
    logger.info("ğŸ”„ æµ‹è¯•4ï¼šæµ‹è¯•æ–­ç‚¹ç»­ä¼ åŠŸèƒ½")
    resume_success = test_resume_functionality()
    print()
    
    # æµ‹è¯•5ï¼šè‚¡ç¥¨æ•°æ®æ”¶é›†æµ‹è¯•
    logger.info("ğŸ“Š æµ‹è¯•5ï¼šæµ‹è¯•è‚¡ç¥¨æ•°æ®æ”¶é›†ï¼ˆ10åªè‚¡ç¥¨ï¼‰")
    collection_success = test_stock_data_collection()
    print()
    
    # æµ‹è¯•ç»“æœæ±‡æ€»
    logger.info("ğŸ“‹ æµ‹è¯•ç»“æœæ±‡æ€»")
    logger.info("=" * 60)
    logger.info(f"âœ… é…ç½®æ£€æŸ¥: é€šè¿‡")
    logger.info(f"âœ… ç¼“å­˜åŠŸèƒ½: é€šè¿‡")
    logger.info(f"âœ… æ•°æ®åº“åŠŸèƒ½: é€šè¿‡")
    logger.info(f"{'âœ…' if resume_success else 'âš ï¸'} æ–­ç‚¹ç»­ä¼ : {'é€šè¿‡' if resume_success else 'éœ€è¦å…ˆæ”¶é›†æ•°æ®'}")
    logger.info(f"{'âœ…' if collection_success else 'âŒ'} æ•°æ®æ”¶é›†: {'é€šè¿‡' if collection_success else 'å¤±è´¥'}")
    
    if collection_success:
        logger.info("\nğŸ‰ æ‰€æœ‰ä¸»è¦åŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")
        logger.info("ğŸ’¡ å»ºè®®ï¼šè¿è¡Œå®Œæ•´çš„ collect_data.py è¿›è¡Œå…¨é‡æ•°æ®æ”¶é›†")
    else:
        logger.error("\nâŒ æ•°æ®æ”¶é›†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥tokené…ç½®å’Œç½‘ç»œè¿æ¥")

if __name__ == "__main__":
    main() 