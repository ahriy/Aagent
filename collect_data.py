import pandas as pd
import tushare as ts
import numpy as np
import time
import sqlite3
from datetime import datetime
import os
from loguru import logger
import argparse
from tqdm import tqdm
import json
from openpyxl import load_workbook
from openpyxl.styles import PatternFill, Font, Alignment
from dotenv import load_dotenv
import config

class TokenManager:
    """ç®¡ç†å¤šä¸ªTushare tokençš„åˆ‡æ¢å’Œé‡è¯•é€»è¾‘"""
    
    def __init__(self, tokens):
        self.tokens = tokens
        self.current_token_index = 0
        self.token_retry_count = {}
        self.total_requests = 0
        self.successful_requests = 0
        self.max_retries_per_token = 3
        self.token_switch_delay = 60  # åˆ‡æ¢tokenåç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
        
        if not tokens:
            raise ValueError("è‡³å°‘éœ€è¦æä¾›ä¸€ä¸ªTushare token")
            
        logger.info(f"ğŸ”§ åˆå§‹åŒ–Tokenç®¡ç†å™¨ï¼Œå…±æœ‰ {len(tokens)} ä¸ªtokenå¯ç”¨")
        self._switch_token()
    
    def _switch_token(self):
        """åˆ‡æ¢åˆ°å½“å‰token"""
        if len(self.tokens) == 1:
            current_token = self.tokens[0]
        else:
            current_token = self.tokens[self.current_token_index]
            
        logger.info(f"ğŸ”„ åˆ‡æ¢åˆ°Token {self.current_token_index + 1}/{len(self.tokens)}")
        ts.set_token(current_token)
        self.pro = ts.pro_api()
        
        # é‡ç½®å½“å‰tokençš„é‡è¯•æ¬¡æ•°
        self.token_retry_count[self.current_token_index] = 0
    
    def _next_token(self):
        """åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªå¯ç”¨token"""
        if len(self.tokens) <= 1:
            return False
            
        # å°è¯•åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªtoken
        original_index = self.current_token_index
        while True:
            self.current_token_index = (self.current_token_index + 1) % len(self.tokens)
            
            # å¦‚æœå›åˆ°åŸå§‹tokenï¼Œè¯´æ˜æ‰€æœ‰tokenéƒ½è¯•è¿‡äº†
            if self.current_token_index == original_index:
                return False
                
            # æ£€æŸ¥è¿™ä¸ªtokenæ˜¯å¦è¿˜æœ‰é‡è¯•æœºä¼š
            retry_count = self.token_retry_count.get(self.current_token_index, 0)
            if retry_count < self.max_retries_per_token:
                self._switch_token()
                time.sleep(self.token_switch_delay)  # åˆ‡æ¢åç­‰å¾…
                return True
        
        return False
    
    def make_request(self, request_func, *args, **kwargs):
        """æ‰§è¡ŒAPIè¯·æ±‚ï¼ŒåŒ…å«é‡è¯•å’Œtokenåˆ‡æ¢é€»è¾‘"""
        self.total_requests += 1
        
        while True:
            try:
                # è®°å½•å½“å‰tokençš„é‡è¯•æ¬¡æ•°
                current_retry = self.token_retry_count.get(self.current_token_index, 0)
                
                if current_retry > 0:
                    logger.warning(f"âš ï¸  Token {self.current_token_index + 1} é‡è¯•ç¬¬ {current_retry} æ¬¡")
                
                # æ‰§è¡Œè¯·æ±‚
                result = request_func(self.pro, *args, **kwargs)
                
                # è¯·æ±‚æˆåŠŸ
                self.successful_requests += 1
                self.token_retry_count[self.current_token_index] = 0  # é‡ç½®é‡è¯•æ¬¡æ•°
                return result
                
            except Exception as e:
                error_msg = str(e)
                self.token_retry_count[self.current_token_index] = current_retry + 1
                
                logger.error(f"âŒ APIè¯·æ±‚å¤±è´¥ (Token {self.current_token_index + 1}, é‡è¯• {current_retry + 1}): {error_msg}")
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯APIé™åˆ¶é”™è¯¯
                if any(keyword in error_msg.lower() for keyword in ['limit', 'é™åˆ¶', 'timeout', 'è¶…æ—¶', 'rate']):
                    logger.warning("ğŸš¦ æ£€æµ‹åˆ°APIé™åˆ¶ï¼Œå°è¯•åˆ‡æ¢token...")
                    
                    # å°è¯•åˆ‡æ¢token
                    if self._next_token():
                        logger.info(f"âœ… å·²åˆ‡æ¢åˆ°Token {self.current_token_index + 1}")
                        continue
                    else:
                        logger.warning("âš ï¸  æ‰€æœ‰tokenéƒ½å·²è¾¾åˆ°é‡è¯•é™åˆ¶ï¼Œç­‰å¾…åé‡ç½®...")
                        time.sleep(self.token_switch_delay * 2)  # ç­‰å¾…æ›´é•¿æ—¶é—´
                        # é‡ç½®æ‰€æœ‰tokençš„é‡è¯•æ¬¡æ•°
                        self.token_retry_count = {}
                        self.current_token_index = 0
                        self._switch_token()
                        continue
                
                # æ£€æŸ¥å½“å‰tokenæ˜¯å¦è¿˜æœ‰é‡è¯•æœºä¼š
                if current_retry < self.max_retries_per_token:
                    wait_time = 2 ** current_retry  # æŒ‡æ•°é€€é¿
                    logger.info(f"â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    continue
                else:
                    # å½“å‰tokené‡è¯•æ¬¡æ•°å·²è¾¾ä¸Šé™ï¼Œå°è¯•åˆ‡æ¢
                    if self._next_token():
                        logger.info(f"âœ… Tokené‡è¯•æ¬¡æ•°è¾¾ä¸Šé™ï¼Œå·²åˆ‡æ¢åˆ°Token {self.current_token_index + 1}")
                        continue
                    else:
                        # æ‰€æœ‰tokenéƒ½è¯•è¿‡äº†ï¼ŒæŠ›å‡ºå¼‚å¸¸
                        raise Exception(f"æ‰€æœ‰tokenéƒ½æ— æ³•å®Œæˆè¯·æ±‚: {error_msg}")
    
    def get_stats(self):
        """è·å–è¯·æ±‚ç»Ÿè®¡ä¿¡æ¯"""
        success_rate = (self.successful_requests / self.total_requests * 100) if self.total_requests > 0 else 0
        return {
            'total_requests': self.total_requests,
            'successful_requests': self.successful_requests,
            'success_rate': f"{success_rate:.1f}%",
            'current_token': self.current_token_index + 1,
            'total_tokens': len(self.tokens)
        }

def check_environment():
    """æ£€æŸ¥ç¯å¢ƒå˜é‡æ˜¯å¦æ­£ç¡®è®¾ç½®"""
    load_dotenv()
    tushare_token = os.getenv('TUSHARE_TOKEN')
    if not tushare_token:
        raise ValueError("è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® TUSHARE_TOKEN")
    return tushare_token

class StockDataCollector:
    def __init__(self, tokens, cache_dir='cache', batch_size=50, use_delay=True):
        # åˆå§‹åŒ–Tokenç®¡ç†å™¨
        self.token_manager = TokenManager(tokens)
        logger.info("Tushare API åˆå§‹åŒ–æˆåŠŸ")
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        self.cache_dir = cache_dir
        self.batch_size = batch_size
        self.use_delay = use_delay  # æ˜¯å¦ä½¿ç”¨å»¶æ—¶
        os.makedirs(cache_dir, exist_ok=True)
        
    def _get_batch_cache_path(self, batch_index):
        """è·å–æ‰¹æ¬¡ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return os.path.join(self.cache_dir, f"batch_{batch_index}.json")
        
    def _save_batch_to_cache(self, batch_data, batch_index):
        """ä¿å­˜æ‰¹æ¬¡æ•°æ®åˆ°ç¼“å­˜"""
        if not batch_data:
            return
            
        cache_path = self._get_batch_cache_path(batch_index)
        try:
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(batch_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ä¿å­˜æ‰¹æ¬¡ç¼“å­˜å¤±è´¥ {cache_path}: {e}")
            
    def _load_batch_from_cache(self, batch_index):
        """ä»ç¼“å­˜åŠ è½½æ‰¹æ¬¡æ•°æ®"""
        cache_path = self._get_batch_cache_path(batch_index)
        if not os.path.exists(cache_path):
            return None
            
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"è¯»å–æ‰¹æ¬¡ç¼“å­˜å¤±è´¥ {cache_path}: {e}")
            return None

    def get_all_stocks(self):
        """è·å–æ‰€æœ‰Aè‚¡ä¸Šå¸‚å…¬å¸åˆ—è¡¨"""
        try:
            # ä»APIè·å–æ•°æ®
            stocks = self.token_manager.make_request(lambda pro: pro.stock_basic(exchange='', list_status='L'))
            return stocks[['ts_code', 'name', 'industry']]
        except Exception as e:
            logger.error(f"è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            return None

    def get_annual_data(self, stock_code, start_year, end_year):
        """è·å–å•ä¸ªè‚¡ç¥¨çš„å¹´åº¦æ•°æ®"""
        try:
            # æ„å»ºå¹´ä»½èŒƒå›´ï¼ˆæ’é™¤å½“å‰å¹´ä»½ï¼Œåªè·å–å®Œæ•´å¹´ä»½çš„æ•°æ®ï¼‰
            current_year = datetime.now().year
            actual_end_year = min(end_year, current_year - 1)  # ä¸åŒ…å«å½“å‰å¹´ä»½
            years = range(start_year, actual_end_year + 1)
            
            # é¢„ç­›é€‰ï¼šæ£€æŸ¥æœ€è¿‘3å¹´æ˜¯å¦è¿ç»­äºæŸ
            recent_years = [actual_end_year - 2, actual_end_year - 1, actual_end_year]
            consecutive_losses = 0
            
            for year in recent_years:
                if year >= start_year:  # ç¡®ä¿å¹´ä»½åœ¨æˆ‘ä»¬çš„æ•°æ®èŒƒå›´å†…
                    year_end = f"{year}1231"
                    try:
                        # è·å–å‡€åˆ©æ¶¦æ•°æ®è¿›è¡Œé¢„ç­›é€‰
                        profit_check = self.token_manager.make_request(
                            lambda pro: pro.fina_indicator(
                                ts_code=stock_code,
                                end_date=year_end,
                                period_type='Y',
                                fields='ts_code,end_date,netprofit_margin'
                            )
                        )
                        if profit_check is not None and not profit_check.empty:
                            year_data = profit_check[profit_check['end_date'].str.startswith(str(year))]
                            if not year_data.empty:
                                net_margin = year_data.iloc[0]['netprofit_margin']
                                if net_margin is not None and net_margin < 0:
                                    consecutive_losses += 1
                        if self.use_delay:
                            time.sleep(0.05)  # å¤§å¹…å‡å°‘å»¶æ—¶ï¼šä»0.1ç§’å‡å°‘åˆ°0.05ç§’
                    except Exception as e:
                        logger.warning(f"é¢„ç­›é€‰æ£€æŸ¥å¤±è´¥ {stock_code} {year}: {e}")
                        # å¦‚æœé¢„ç­›é€‰å¤±è´¥ï¼Œç»§ç»­å¤„ç†ï¼Œä¸è·³è¿‡
                        break
            
            # å¦‚æœæœ€è¿‘3å¹´è¿ç»­äºæŸï¼Œè·³è¿‡æ­¤è‚¡ç¥¨
            if consecutive_losses >= 3:
                logger.info(f"è·³è¿‡è¿ç»­äºæŸè‚¡ç¥¨ {stock_code}ï¼Œè¿ç»­äºæŸ{consecutive_losses}å¹´")
                return None
            
            data = {
                'financial_indicators': [],
                'balance_sheet': [],
                'dividend': [],
                'pe': [],
                'pb': [],
                'cashflow': []
            }
            
            # è·å–æ¯å¹´çš„å¹´æŠ¥è´¢åŠ¡æŒ‡æ ‡
            for year in years:
                # è·å–å¹´æŠ¥è´¢åŠ¡æŒ‡æ ‡ï¼ˆä½¿ç”¨å¹´æœ«æ—¥æœŸï¼‰
                year_end = f"{year}1231"
                
                # 1. ä¸»è¦è´¢åŠ¡æŒ‡æ ‡
                indicators = self.token_manager.make_request(
                    lambda pro: pro.fina_indicator(
                        ts_code=stock_code,
                        end_date=year_end,
                        period_type='Y',
                        fields='ts_code,end_date,roe,grossprofit_margin,netprofit_margin,debt_to_assets,current_ratio,assets_turn'
                    )
                )
                if indicators is not None and not indicators.empty:
                    # è¿‡æ»¤å‡ºè¯¥å¹´çš„å¹´æŠ¥æ•°æ®
                    year_indicators = indicators[indicators['end_date'].str.startswith(str(year))]
                    if not year_indicators.empty:
                        # åªå–æœ€æ–°çš„ä¸€æ¡å¹´æŠ¥æ•°æ®
                        latest_indicator = year_indicators.iloc[0:1]
                        data['financial_indicators'].extend(latest_indicator.to_dict('records'))
                
                # 2. èµ„äº§è´Ÿå€ºè¡¨æ•°æ®ï¼ˆè·å–è¥æ”¶ç­‰ï¼‰
                balance_sheet = self.token_manager.make_request(
                    lambda pro: pro.balancesheet(
                        ts_code=stock_code,
                        end_date=year_end,
                        period_type='Y',
                        fields='ts_code,end_date,total_assets'
                    )
                )
                if balance_sheet is not None and not balance_sheet.empty:
                    year_balance = balance_sheet[balance_sheet['end_date'].str.startswith(str(year))]
                    if not year_balance.empty:
                        data['balance_sheet'].extend(year_balance.iloc[0:1].to_dict('records'))
                
                # 3. ç°é‡‘æµé‡è¡¨æ•°æ®
                cashflow = self.token_manager.make_request(
                    lambda pro: pro.cashflow(
                        ts_code=stock_code,
                        end_date=year_end,
                        period_type='Y',
                        fields='ts_code,end_date,n_cashflow_act,net_profit'
                    )
                )
                if cashflow is not None and not cashflow.empty:
                    year_cashflow = cashflow[cashflow['end_date'].str.startswith(str(year))]
                    if not year_cashflow.empty:
                        data['cashflow'].extend(year_cashflow.iloc[0:1].to_dict('records'))
                
                # 4. è·å–å¹´æœ«è‚¡æ¯ç‡ï¼ˆå°è¯•å¤šä¸ªæ—¥æœŸï¼‰
                dividend_found = False
                for month_day in ['1231', '1230', '1229', '1228']:  # å°è¯•å¹´æœ«å‡ å¤©
                    test_date = f"{year}{month_day}"
                    dividend = self.token_manager.make_request(
                        lambda pro: pro.daily_basic(
                            ts_code=stock_code,
                            trade_date=test_date,
                            fields='ts_code,trade_date,dv_ratio'
                        )
                    )
                    if dividend is not None and not dividend.empty:
                        data['dividend'].extend(dividend.to_dict('records'))
                        dividend_found = True
                        break
                    if self.use_delay:
                        time.sleep(0.02)  # å¤§å¹…å‡å°‘å»¶æ—¶
                
                # 5. è·å–å¹´æœ«PEï¼ˆå°è¯•å¤šä¸ªæ—¥æœŸï¼‰
                pe_found = False
                for month_day in ['1231', '1230', '1229', '1228']:  # å°è¯•å¹´æœ«å‡ å¤©
                    test_date = f"{year}{month_day}"
                    pe = self.token_manager.make_request(
                        lambda pro: pro.daily_basic(
                            ts_code=stock_code,
                            trade_date=test_date,
                            fields='ts_code,trade_date,pe'
                        )
                    )
                    if pe is not None and not pe.empty:
                        data['pe'].extend(pe.to_dict('records'))
                        pe_found = True
                        break
                    if self.use_delay:
                        time.sleep(0.02)  # å¤§å¹…å‡å°‘å»¶æ—¶
                
                # 6. è·å–å¹´æœ«PBï¼ˆå°è¯•å¤šä¸ªæ—¥æœŸï¼‰
                pb_found = False
                for month_day in ['1231', '1230', '1229', '1228']:  # å°è¯•å¹´æœ«å‡ å¤©
                    test_date = f"{year}{month_day}"
                    pb = self.token_manager.make_request(
                        lambda pro: pro.daily_basic(
                            ts_code=stock_code,
                            trade_date=test_date,
                            fields='ts_code,trade_date,pb'
                        )
                    )
                    if pb is not None and not pb.empty:
                        data['pb'].extend(pb.to_dict('records'))
                        pb_found = True
                        break
                    if self.use_delay:
                        time.sleep(0.02)  # å¤§å¹…å‡å°‘å»¶æ—¶
                
                if self.use_delay:
                    time.sleep(0.1)  # æ¯å¹´æ•°æ®é—´éš”ï¼šä»0.3ç§’å‡å°‘åˆ°0.1ç§’
            
            return data
            
        except Exception as e:
            logger.error(f"è·å–å¹´åº¦æ•°æ®å¤±è´¥ {stock_code}: {e}")
            return None

    def process_batch(self, stocks_batch, start_year, end_year, use_cache=True):
        """å¤„ç†ä¸€æ‰¹è‚¡ç¥¨æ•°æ®"""
        batch_index = stocks_batch.index[0] // self.batch_size
        
        # å°è¯•ä»ç¼“å­˜åŠ è½½
        if use_cache:
            cached_data = self._load_batch_from_cache(batch_index)
            if cached_data is not None:
                return cached_data
        
        # è·å–æ–°æ•°æ®
        batch_data = {}
        for _, stock in stocks_batch.iterrows():
            stock_code = stock['ts_code']
            stock_data = self.get_annual_data(stock_code, start_year, end_year)
            if stock_data:
                batch_data[stock_code] = {
                    'name': stock['name'],
                    'industry': stock['industry'],
                    'data': stock_data
                }
        
        # ä¿å­˜åˆ°ç¼“å­˜
        if batch_data:
            self._save_batch_to_cache(batch_data, batch_index)
        
        return batch_data

class ExcelOptimizer:
    """Excelæ•°æ®ä¼˜åŒ–å™¨"""
    def __init__(self, df):
        self.df = df
        
    def create_summary_view(self):
        """åˆ›å»ºæ±‡æ€»è§†å›¾ - åªæ˜¾ç¤ºå…³é”®ä¿¡æ¯"""
        if self.df is None:
            return None
            
        # åŸºæœ¬ä¿¡æ¯åˆ—
        basic_cols = ['stock_code', 'stock_name', 'industry', 'need_analysis']
        
        # è®¡ç®—å„æŒ‡æ ‡çš„æœ€æ–°å€¼å’Œå¹³å‡å€¼
        summary_data = []
        
        for _, row in self.df.iterrows():
            summary_row = {}
            
            # åŸºæœ¬ä¿¡æ¯
            for col in basic_cols:
                if col in row:
                    summary_row[col] = row[col]
            
            # ROEæ±‡æ€»
            roe_cols = [col for col in self.df.columns if col.startswith('roe_')]
            roe_values = [row[col] for col in roe_cols if pd.notna(row[col])]
            if roe_values:
                summary_row['roe_æœ€æ–°'] = roe_values[-1]
                summary_row['roe_å¹³å‡'] = np.mean(roe_values)
                summary_row['roe_è¶‹åŠ¿'] = 'ä¸Šå‡' if len(roe_values) > 1 and roe_values[-1] > roe_values[0] else 'ä¸‹é™'
            
            # æ¯›åˆ©ç‡æ±‡æ€»
            gm_cols = [col for col in self.df.columns if col.startswith('gross_margin_')]
            gm_values = [row[col] for col in gm_cols if pd.notna(row[col])]
            if gm_values:
                summary_row['æ¯›åˆ©ç‡_æœ€æ–°'] = gm_values[-1]
                summary_row['æ¯›åˆ©ç‡_å¹³å‡'] = np.mean(gm_values)
            
            # å‡€åˆ©ç‡æ±‡æ€»
            nm_cols = [col for col in self.df.columns if col.startswith('net_margin_')]
            nm_values = [row[col] for col in nm_cols if pd.notna(row[col])]
            if nm_values:
                summary_row['å‡€åˆ©ç‡_æœ€æ–°'] = nm_values[-1]
                summary_row['å‡€åˆ©ç‡_å¹³å‡'] = np.mean(nm_values)
            
            # PEæ±‡æ€»
            pe_cols = [col for col in self.df.columns if col.startswith('pe_')]
            pe_values = [row[col] for col in pe_cols if pd.notna(row[col])]
            if pe_values:
                summary_row['PE_æœ€æ–°'] = pe_values[-1]
                summary_row['PE_å¹³å‡'] = np.mean(pe_values)
            
            # è‚¡æ¯ç‡æ±‡æ€»
            div_cols = [col for col in self.df.columns if col.startswith('dividend_')]
            div_values = [row[col] for col in div_cols if pd.notna(row[col])]
            if div_values:
                summary_row['è‚¡æ¯ç‡_æœ€æ–°'] = div_values[-1]
                summary_row['è‚¡æ¯ç‡_å¹³å‡'] = np.mean(div_values)
            
            # ç»¼åˆè¯„åˆ†ï¼ˆç®€å•è¯„åˆ†é€»è¾‘ï¼‰
            score = 0
            if 'roe_å¹³å‡' in summary_row and summary_row['roe_å¹³å‡'] > 15:
                score += 20
            if 'æ¯›åˆ©ç‡_å¹³å‡' in summary_row and summary_row['æ¯›åˆ©ç‡_å¹³å‡'] > 30:
                score += 20
            if 'å‡€åˆ©ç‡_å¹³å‡' in summary_row and summary_row['å‡€åˆ©ç‡_å¹³å‡'] > 10:
                score += 20
            if 'PE_å¹³å‡' in summary_row and 10 < summary_row['PE_å¹³å‡'] < 25:
                score += 20
            if 'è‚¡æ¯ç‡_å¹³å‡' in summary_row and summary_row['è‚¡æ¯ç‡_å¹³å‡'] > 2:
                score += 20
            
            summary_row['ç»¼åˆè¯„åˆ†'] = score
            summary_data.append(summary_row)
        
        return pd.DataFrame(summary_data)
    
    def create_sector_analysis(self):
        """åˆ›å»ºè¡Œä¸šåˆ†æè§†å›¾"""
        if self.df is None:
            return None
            
        # æŒ‰è¡Œä¸šåˆ†ç»„ç»Ÿè®¡
        sector_stats = []
        
        for industry in self.df['industry'].unique():
            if pd.isna(industry):
                continue
                
            industry_data = self.df[self.df['industry'] == industry]
            
            # è®¡ç®—è¡Œä¸šå¹³å‡æŒ‡æ ‡
            roe_cols = [col for col in self.df.columns if col.startswith('roe_')]
            pe_cols = [col for col in self.df.columns if col.startswith('pe_')]
            
            sector_row = {
                'è¡Œä¸š': industry,
                'å…¬å¸æ•°é‡': len(industry_data),
                'å¹³å‡ROE': industry_data[roe_cols].mean().mean(),
                'å¹³å‡PE': industry_data[pe_cols].mean().mean(),
                'é«˜ROEå…¬å¸æ•°': (industry_data[roe_cols].mean(axis=1) > 15).sum(),
                'éœ€è¦åˆ†ææ•°': (industry_data['need_analysis'] == True).sum()
            }
            sector_stats.append(sector_row)
        
        return pd.DataFrame(sector_stats).sort_values('å¹³å‡ROE', ascending=False)
    
    def create_filtered_views(self):
        """åˆ›å»ºç­›é€‰è§†å›¾"""
        if self.df is None:
            return {}
            
        views = {}
        
        # é«˜ROEè‚¡ç¥¨ï¼ˆROEå‡å€¼>15%ï¼‰
        roe_cols = [col for col in self.df.columns if col.startswith('roe_')]
        high_roe_mask = self.df[roe_cols].mean(axis=1) > 15
        views['é«˜ROEè‚¡ç¥¨'] = self.df[high_roe_mask][['stock_code', 'stock_name', 'industry'] + roe_cols]
        
        # ä½PEè‚¡ç¥¨ï¼ˆPEå‡å€¼<20ï¼‰
        pe_cols = [col for col in self.df.columns if col.startswith('pe_')]
        low_pe_mask = self.df[pe_cols].mean(axis=1) < 20
        views['ä½PEè‚¡ç¥¨'] = self.df[low_pe_mask][['stock_code', 'stock_name', 'industry'] + pe_cols]
        
        # é«˜è‚¡æ¯è‚¡ç¥¨ï¼ˆè‚¡æ¯ç‡å‡å€¼>3%ï¼‰
        div_cols = [col for col in self.df.columns if col.startswith('dividend_')]
        high_div_mask = self.df[div_cols].mean(axis=1) > 3
        views['é«˜è‚¡æ¯è‚¡ç¥¨'] = self.df[high_div_mask][['stock_code', 'stock_name', 'industry'] + div_cols]
        
        # ç¨³å®šç›ˆåˆ©è‚¡ç¥¨ï¼ˆå‡€åˆ©ç‡è¿ç»­æ­£å€¼ï¼‰
        nm_cols = [col for col in self.df.columns if col.startswith('net_margin_')]
        stable_profit_mask = (self.df[nm_cols] > 0).all(axis=1)
        views['ç¨³å®šç›ˆåˆ©è‚¡ç¥¨'] = self.df[stable_profit_mask][['stock_code', 'stock_name', 'industry'] + nm_cols]
        
        return views
    
    def _apply_styles(self, excel_file):
        """åº”ç”¨Excelæ ·å¼"""
        try:
            wb = load_workbook(excel_file)
            
            # ä¸ºæ¯ä¸ªå·¥ä½œè¡¨æ·»åŠ æ ·å¼
            for sheet_name in wb.sheetnames:
                ws = wb[sheet_name]
                
                # è®¾ç½®æ ‡é¢˜è¡Œæ ·å¼
                header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
                header_font = Font(color="FFFFFF", bold=True)
                
                for col in range(1, ws.max_column + 1):
                    cell = ws.cell(row=1, column=col)
                    cell.fill = header_fill
                    cell.font = header_font
                    cell.alignment = Alignment(horizontal="center")
                
                # è®¾ç½®æ•°æ®è¡Œæ ·å¼
                for row in range(2, ws.max_row + 1):
                    for col in range(1, ws.max_column + 1):
                        cell = ws.cell(row=row, column=col)
                        cell.alignment = Alignment(horizontal="center")
                        
                        # ä¸ºæ•°å€¼ç±»å‹çš„å•å…ƒæ ¼è®¾ç½®æ¡ä»¶æ ¼å¼
                        if isinstance(cell.value, (int, float)):
                            if cell.value is not None and cell.value < 0:
                                cell.font = Font(color="FF0000")  # è´Ÿå€¼æ˜¾ç¤ºçº¢è‰²
                            elif cell.value is not None and cell.value > 20:
                                cell.font = Font(color="008000")  # é«˜å€¼æ˜¾ç¤ºç»¿è‰²
                
                # è‡ªåŠ¨è°ƒæ•´åˆ—å®½
                for column in ws.columns:
                    max_length = 0
                    column_letter = column[0].column_letter
                    for cell in column:
                        try:
                            if len(str(cell.value)) > max_length:
                                max_length = len(str(cell.value))
                        except:
                            pass
                    adjusted_width = min(max_length + 2, 20)
                    ws.column_dimensions[column_letter].width = adjusted_width
            
            wb.save(excel_file)
            
        except Exception as e:
            logger.error(f"åº”ç”¨æ ·å¼å¤±è´¥: {e}")
    
    def generate_analysis_suggestions(self):
        """ç”Ÿæˆåˆ†æå»ºè®®"""
        if self.df is None:
            return None
            
        suggestions = []
        
        # 1. é«˜ä»·å€¼è‚¡ç¥¨æ¨è
        summary_df = self.create_summary_view()
        if summary_df is not None:
            high_score_stocks = summary_df[summary_df['ç»¼åˆè¯„åˆ†'] >= 80].sort_values('ç»¼åˆè¯„åˆ†', ascending=False)
            if not high_score_stocks.empty:
                suggestions.append("ğŸŒŸ é«˜ä»·å€¼è‚¡ç¥¨æ¨èï¼š")
                for _, stock in high_score_stocks.head(10).iterrows():
                    suggestions.append(f"  â€¢ {stock['stock_name']}({stock['stock_code']}) - è¯„åˆ†:{stock['ç»¼åˆè¯„åˆ†']}")
        
        # 2. è¡Œä¸šæœºä¼š
        sector_df = self.create_sector_analysis()
        if sector_df is not None:
            top_sectors = sector_df.head(5)
            suggestions.append("\nğŸ“ˆ ä¼˜åŠ¿è¡Œä¸šï¼š")
            for _, sector in top_sectors.iterrows():
                suggestions.append(f"  â€¢ {sector['è¡Œä¸š']} - å¹³å‡ROE:{sector['å¹³å‡ROE']:.2f}%")
        
        # 3. ç­›é€‰å»ºè®®
        filtered_views = self.create_filtered_views()
        suggestions.append("\nğŸ” ç­›é€‰å»ºè®®ï¼š")
        for view_name, view_df in filtered_views.items():
            suggestions.append(f"  â€¢ {view_name}: {len(view_df)}åªè‚¡ç¥¨")
        
        return '\n'.join(suggestions)
    
    def save_optimized_excel(self, output_file='stock_analysis_optimized.xlsx'):
        """ä¿å­˜ä¼˜åŒ–åçš„Excelæ–‡ä»¶"""
        if self.df is None:
            logger.error("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return False
            
        try:
            with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
                # 1. æ±‡æ€»è§†å›¾
                summary_df = self.create_summary_view()
                if summary_df is not None:
                    summary_df.to_excel(writer, sheet_name='æ±‡æ€»è§†å›¾', index=False)
                    logger.info(f"æ±‡æ€»è§†å›¾å·²åˆ›å»ºï¼ŒåŒ…å«{len(summary_df)}è¡Œæ•°æ®")
                
                # 2. è¡Œä¸šåˆ†æ
                sector_df = self.create_sector_analysis()
                if sector_df is not None:
                    sector_df.to_excel(writer, sheet_name='è¡Œä¸šåˆ†æ', index=False)
                    logger.info(f"è¡Œä¸šåˆ†æå·²åˆ›å»ºï¼ŒåŒ…å«{len(sector_df)}ä¸ªè¡Œä¸š")
                
                # 3. ç­›é€‰è§†å›¾
                filtered_views = self.create_filtered_views()
                for view_name, view_df in filtered_views.items():
                    if not view_df.empty:
                        view_df.to_excel(writer, sheet_name=view_name, index=False)
                        logger.info(f"{view_name}å·²åˆ›å»ºï¼ŒåŒ…å«{len(view_df)}åªè‚¡ç¥¨")
                
                # 4. åŸå§‹æ•°æ®ï¼ˆå¯é€‰ï¼‰
                self.df.to_excel(writer, sheet_name='åŸå§‹æ•°æ®', index=False)
                logger.info("åŸå§‹æ•°æ®å·²ä¿ç•™")
            
            # æ·»åŠ æ ·å¼
            self._apply_styles(output_file)
            logger.info(f"ä¼˜åŒ–åçš„Excelæ–‡ä»¶å·²ä¿å­˜: {output_file}")
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜Excelæ–‡ä»¶å¤±è´¥: {e}")
            return False

def setup_logger():
    """é…ç½®æ—¥å¿—"""
    log_path = "logs"
    os.makedirs(log_path, exist_ok=True)
    logger.add(
        os.path.join(log_path, "data_collection_{time}.log"),
        rotation="500 MB",
        encoding="utf-8"
    )

def process_stock_data(batch_data):
    """å¤„ç†è‚¡ç¥¨æ•°æ®ä¸ºæœ€ç»ˆæ ¼å¼"""
    results = []
    for stock_code, stock_info in batch_data.items():
        row = {
            'stock_code': stock_code,
            'stock_name': stock_info['name'],
            'industry': stock_info['industry'],
            'need_analysis': False
        }
        
        # å¤„ç†è´¢åŠ¡æŒ‡æ ‡
        for indicator in stock_info['data']['financial_indicators']:
            year = indicator['end_date'][:4]
            row[f'roe_{year}'] = indicator['roe']
            row[f'gross_margin_{year}'] = indicator['grossprofit_margin']
            row[f'net_margin_{year}'] = indicator['netprofit_margin']
            row[f'debt_ratio_{year}'] = indicator['debt_to_assets']
            row[f'current_ratio_{year}'] = indicator['current_ratio']
            row[f'asset_turnover_{year}'] = indicator['assets_turn']
        
        # å¤„ç†èµ„äº§è´Ÿå€ºè¡¨æ•°æ®
        for balance in stock_info['data']['balance_sheet']:
            year = balance['end_date'][:4]
            row[f'total_assets_{year}'] = balance['total_assets']
        
        # å¤„ç†ç°é‡‘æµæ•°æ®
        for cf in stock_info['data']['cashflow']:
            year = cf['end_date'][:4]
            # è®¡ç®—ç°é‡‘æµè´¨é‡æ¯”ç‡ï¼ˆç»è¥ç°é‡‘æµ/å‡€åˆ©æ¶¦ï¼‰
            if cf['n_cashflow_act'] and cf['net_profit'] and cf['net_profit'] != 0:
                row[f'ocf_to_profit_{year}'] = cf['n_cashflow_act'] / cf['net_profit']
        
        # å¤„ç†è‚¡æ¯ç‡
        for dividend in stock_info['data']['dividend']:
            year = dividend['trade_date'][:4]
            row[f'dividend_{year}'] = dividend['dv_ratio']
        
        # å¤„ç†PE
        for pe_data in stock_info['data']['pe']:
            year = pe_data['trade_date'][:4]
            row[f'pe_{year}'] = pe_data['pe']
        
        # å¤„ç†PB
        for pb_data in stock_info['data']['pb']:
            year = pb_data['trade_date'][:4]
            row[f'pb_{year}'] = pb_data['pb']
        
        results.append(row)
    
    return results

def create_sqlite_database(db_path='stock_analysis.db'):
    """åˆ›å»ºSQLiteæ•°æ®åº“å’Œè¡¨ç»“æ„"""
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    # åˆ›å»ºè‚¡ç¥¨åŸºæœ¬ä¿¡æ¯è¡¨
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS stocks (
            stock_code TEXT PRIMARY KEY,
            stock_name TEXT,
            industry TEXT,
            list_date TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # åˆ›å»ºè´¢åŠ¡æŒ‡æ ‡è¡¨ï¼ˆé•¿æ ¼å¼ï¼Œä¾¿äºæŸ¥è¯¢ï¼‰
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS financial_metrics (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            stock_code TEXT,
            year INTEGER,
            metric_name TEXT,
            metric_value REAL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (stock_code) REFERENCES stocks (stock_code)
        )
    ''')
    
    # åˆ›å»ºç´¢å¼•æé«˜æŸ¥è¯¢æ€§èƒ½
    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_stock_year 
        ON financial_metrics (stock_code, year)
    ''')
    
    cursor.execute('''
        CREATE INDEX IF NOT EXISTS idx_metric_name 
        ON financial_metrics (metric_name)
    ''')
    
    conn.commit()
    conn.close()
    logger.info(f"SQLiteæ•°æ®åº“å·²åˆ›å»º: {db_path}")

def save_to_sqlite(data, db_path='stock_analysis.db'):
    """ä¿å­˜æ•°æ®åˆ°SQLiteæ•°æ®åº“"""
    conn = sqlite3.connect(db_path)
    
    for _, row in data.iterrows():
        # æ’å…¥è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
        conn.execute('''
            INSERT OR REPLACE INTO stocks (stock_code, stock_name, industry)
            VALUES (?, ?, ?)
        ''', (row['stock_code'], row['stock_name'], row['industry']))
        
        # æ’å…¥è´¢åŠ¡æŒ‡æ ‡æ•°æ®
        for col in row.index:
            if col in ['stock_code', 'stock_name', 'industry', 'need_analysis']:
                continue
                
            # è§£ææŒ‡æ ‡åç§°å’Œå¹´ä»½
            if '_' in col:
                parts = col.split('_')
                if len(parts) >= 2:
                    metric_name = '_'.join(parts[:-1])
                    year = parts[-1]
                    
                    if pd.notna(row[col]) and year.isdigit():
                        conn.execute('''
                            INSERT OR REPLACE INTO financial_metrics 
                            (stock_code, year, metric_name, metric_value)
                            VALUES (?, ?, ?, ?)
                        ''', (row['stock_code'], int(year), metric_name, float(row[col])))
    
    conn.commit()
    conn.close()
    logger.info(f"æ•°æ®å·²ä¿å­˜åˆ°SQLiteæ•°æ®åº“: {db_path}")

def main():
    """ä¸»ç¨‹åºå…¥å£"""
    setup_logger()
    
    # åˆ›å»ºSQLiteæ•°æ®åº“
    create_sqlite_database()
    
    # å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(description='Aè‚¡åŸºæœ¬é¢æ•°æ®æ”¶é›†å·¥å…· - æ”¯æŒå¤šToken')
    parser.add_argument('--limit', type=int, default=None, help='é™åˆ¶å¤„ç†çš„è‚¡ç¥¨æ•°é‡ï¼ˆæµ‹è¯•ç”¨ï¼‰')
    parser.add_argument('--batch-size', type=int, default=50, help='æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--no-cache', action='store_true', help='ä¸ä½¿ç”¨ç¼“å­˜ï¼Œé‡æ–°è·å–æ•°æ®')
    parser.add_argument('--start-year', type=int, default=2019, help='å¼€å§‹å¹´ä»½')
    parser.add_argument('--end-year', type=int, default=2023, help='ç»“æŸå¹´ä»½')
    parser.add_argument('--no-optimize', action='store_true', help='ä¸ç”Ÿæˆä¼˜åŒ–Excelè§†å›¾')
    parser.add_argument('--no-delay', action='store_true', help='ä¸ä½¿ç”¨å»¶æ—¶ï¼Œæœ€å¿«é€Ÿåº¦è¿è¡Œï¼ˆå¯èƒ½è§¦å‘APIé™åˆ¶ï¼‰')
    parser.add_argument('--no-realtime-db', action='store_true', help='ä¸å®æ—¶æ›´æ–°æ•°æ®åº“ï¼Œä»…åœ¨æœ€åç»Ÿä¸€ä¿å­˜')
    
    args = parser.parse_args()
    
    # ä»é…ç½®æ–‡ä»¶è·å–æ‰€æœ‰token
    tokens = []
    if config.TUSHARE_TOKENS:
        tokens = config.TUSHARE_TOKENS
    elif config.TUSHARE_TOKEN:
        tokens = [config.TUSHARE_TOKEN]
        
    if not tokens:
        logger.error("è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®TUSHARE_TOKENæˆ–TUSHARE_TOKENS")
        return
    
    logger.info(f"ğŸ”§ é…ç½®çš„Tokenæ•°é‡: {len(tokens)}")
    
    # åˆå§‹åŒ–æ”¶é›†å™¨ï¼ˆä¼ å…¥æ‰€æœ‰tokensï¼‰
    collector = StockDataCollector(
        tokens,  # ä¼ å…¥æ‰€æœ‰tokens
        cache_dir='cache', 
        batch_size=args.batch_size,
        use_delay=not args.no_delay  # å¦‚æœæŒ‡å®šäº†no_delayï¼Œåˆ™ä¸ä½¿ç”¨å»¶æ—¶
    )
    
    try:
        logger.info(f"æ•°æ®æ”¶é›†æ—¶é—´èŒƒå›´ï¼š{args.start_year} è‡³ {args.end_year}")
        
        # è·å–è‚¡ç¥¨åˆ—è¡¨
        stocks = collector.get_all_stocks()
        if stocks is None:
            logger.error("è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥")
            return
        
        # é™åˆ¶è‚¡ç¥¨æ•°é‡ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰
        if args.limit:
            stocks = stocks.head(args.limit)
            logger.info(f"é™åˆ¶æ¨¡å¼ï¼šåªå¤„ç†å‰ {args.limit} åªè‚¡ç¥¨")
        
        logger.info(f"å…±è·å–åˆ° {len(stocks)} åªè‚¡ç¥¨")
        
        # è®¡ç®—æ‰¹æ¬¡æ•°é‡
        total_batches = (len(stocks) + args.batch_size - 1) // args.batch_size
        logger.info(f"å°†åˆ† {total_batches} ä¸ªæ‰¹æ¬¡å¤„ç†")
        
        all_results = []
        
        # æŒ‰æ‰¹æ¬¡å¤„ç†
        for i in tqdm(range(total_batches), desc="å¤„ç†æ‰¹æ¬¡"):
            start_idx = i * args.batch_size
            end_idx = min((i + 1) * args.batch_size, len(stocks))
            stocks_batch = stocks.iloc[start_idx:end_idx]
            
            # å¤„ç†å½“å‰æ‰¹æ¬¡
            batch_data = collector.process_batch(
                stocks_batch,
                args.start_year,
                args.end_year,
                use_cache=not args.no_cache
            )
            
            # å¤„ç†æ•°æ®å¹¶æ·»åŠ åˆ°ç»“æœ
            if batch_data:
                batch_results = process_stock_data(batch_data)
                all_results.extend(batch_results)
                
                # ğŸ”„ å®æ—¶ä¿å­˜å½“å‰æ‰¹æ¬¡åˆ°æ•°æ®åº“
                if batch_results:
                    batch_df = pd.DataFrame(batch_results)
                    if not args.no_realtime_db:
                        save_to_sqlite(batch_df)
                        logger.info(f"âœ… æ‰¹æ¬¡ {i+1} æ•°æ®å·²ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆ{len(batch_results)}åªè‚¡ç¥¨ï¼‰")
                    else:
                        logger.info(f"ğŸ“¦ æ‰¹æ¬¡ {i+1} æ•°æ®å·²ç¼“å­˜ï¼ˆ{len(batch_results)}åªè‚¡ç¥¨ï¼‰ï¼Œå°†åœ¨æœ€åç»Ÿä¸€ä¿å­˜")
                
                logger.info(f"å®Œæˆç¬¬ {i+1}/{total_batches} æ‰¹æ¬¡å¤„ç†ï¼Œå½“å‰å·²å¤„ç† {len(all_results)} åªè‚¡ç¥¨")
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        if all_results:
            df = pd.DataFrame(all_results)
            output_file = 'stock_analysis_data.xlsx'
            df.to_excel(output_file, index=False)
            logger.info(f"åŸå§‹æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
            
            # æ˜¾ç¤ºè¿‡æ»¤æ•ˆæœç»Ÿè®¡
            total_attempted = len(stocks)
            successfully_processed = len(all_results)
            filtered_out = total_attempted - successfully_processed
            filter_rate = (filtered_out / total_attempted * 100) if total_attempted > 0 else 0
            
            # æ˜¾ç¤ºTokenä½¿ç”¨ç»Ÿè®¡
            token_stats = collector.token_manager.get_stats()
            
            logger.info(f"ğŸ“Š æ•°æ®å¤„ç†ç»Ÿè®¡:")
            logger.info(f"  â€¢ æ€»è‚¡ç¥¨æ•°: {total_attempted}")
            logger.info(f"  â€¢ æˆåŠŸå¤„ç†: {successfully_processed}")
            logger.info(f"  â€¢ è¿‡æ»¤æ‰æ•°: {filtered_out} ({filter_rate:.1f}%)")
            logger.info(f"  â€¢ æ•°æ®åˆ—æ•°: {len(df.columns)}")
            
            logger.info(f"ğŸ”§ Tokenä½¿ç”¨ç»Ÿè®¡:")
            logger.info(f"  â€¢ æ€»è¯·æ±‚æ•°: {token_stats['total_requests']}")
            logger.info(f"  â€¢ æˆåŠŸè¯·æ±‚: {token_stats['successful_requests']}")
            logger.info(f"  â€¢ æˆåŠŸç‡: {token_stats['success_rate']}")
            logger.info(f"  â€¢ å½“å‰Token: {token_stats['current_token']}/{token_stats['total_tokens']}")
            
            # è‡ªåŠ¨ç”Ÿæˆä¼˜åŒ–è§†å›¾
            if not args.no_optimize:
                logger.info("å¼€å§‹ç”Ÿæˆä¼˜åŒ–Excelè§†å›¾...")
                optimizer = ExcelOptimizer(df)
                
                if optimizer.save_optimized_excel():
                    logger.info("âœ… ä¼˜åŒ–Excelæ–‡ä»¶åˆ›å»ºæˆåŠŸ: stock_analysis_optimized.xlsx")
                    
                    # ç”Ÿæˆåˆ†æå»ºè®®
                    suggestions = optimizer.generate_analysis_suggestions()
                    if suggestions:
                        logger.info("ğŸ“‹ æŠ•èµ„åˆ†æå»ºè®®ï¼š")
                        print("\n" + suggestions)
                        
                        # ä¿å­˜å»ºè®®åˆ°æ–‡ä»¶
                        with open('analysis_suggestions.txt', 'w', encoding='utf-8') as f:
                            f.write(suggestions)
                        logger.info("ğŸ’¾ åˆ†æå»ºè®®å·²ä¿å­˜åˆ°: analysis_suggestions.txt")
                    
                    print(f"\nğŸ¯ æ•°æ®å¤„ç†å®Œæˆï¼ç”Ÿæˆäº†ä»¥ä¸‹æ–‡ä»¶ï¼š")
                    print(f"  ğŸ“„ {output_file} - åŸå§‹æ•°æ® ({len(df.columns)}åˆ—)")
                    print(f"  ğŸ“Š stock_analysis_optimized.xlsx - ä¼˜åŒ–è§†å›¾ (7ä¸ªå·¥ä½œè¡¨)")
                    print(f"  ğŸ“ analysis_suggestions.txt - æŠ•èµ„å»ºè®®")
                    print(f"\nğŸ”§ Tokenç»Ÿè®¡: {token_stats['success_rate']} æˆåŠŸç‡ï¼Œä½¿ç”¨äº† {token_stats['total_tokens']} ä¸ªToken")
                else:
                    logger.error("ä¼˜åŒ–Excelæ–‡ä»¶åˆ›å»ºå¤±è´¥")
            else:
                logger.info("å·²è·³è¿‡ä¼˜åŒ–Excelè§†å›¾ç”Ÿæˆï¼ˆä½¿ç”¨--no-optimizeå‚æ•°ï¼‰")
                
            # æ•°æ®åº“ä¿å­˜é€»è¾‘
            if args.no_realtime_db:
                # ç»Ÿä¸€ä¿å­˜æ‰€æœ‰æ•°æ®åˆ°æ•°æ®åº“
                save_to_sqlite(df)
                logger.info("ğŸ“Š æ‰€æœ‰æ•°æ®å·²ç»Ÿä¸€ä¿å­˜åˆ°SQLiteæ•°æ®åº“")
            else:
                # æ•°æ®å·²åœ¨æ‰¹æ¬¡å¤„ç†æ—¶å®æ—¶ä¿å­˜åˆ°æ•°æ®åº“
                logger.info("ğŸ“Š æ‰€æœ‰æ‰¹æ¬¡æ•°æ®å·²å®æ—¶ä¿å­˜åˆ°SQLiteæ•°æ®åº“")
            
        else:
            logger.error("æ²¡æœ‰æ”¶é›†åˆ°ä»»ä½•æ•°æ®")
            
    except Exception as e:
        logger.error(f"ä¸»ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        # æ˜¾ç¤ºTokenç»Ÿè®¡ï¼ˆå³ä½¿å‡ºé”™ä¹Ÿæ˜¾ç¤ºï¼‰
        try:
            token_stats = collector.token_manager.get_stats()
            logger.info(f"ğŸ”§ æœ€ç»ˆTokenç»Ÿè®¡: {token_stats}")
        except:
            pass

if __name__ == "__main__":
    main() 