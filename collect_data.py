import tushare as ts
import pandas as pd
from datetime import datetime, timedelta
from loguru import logger
import os
import json
import time
from tqdm import tqdm
from dotenv import load_dotenv
import argparse
import math
import numpy as np
from openpyxl import load_workbook
from openpyxl.styles import PatternFill, Font, Alignment
import config

def check_environment():
    """æ£€æŸ¥ç¯å¢ƒå˜é‡æ˜¯å¦æ­£ç¡®è®¾ç½®"""
    load_dotenv()
    tushare_token = os.getenv('TUSHARE_TOKEN')
    if not tushare_token:
        raise ValueError("è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® TUSHARE_TOKEN")
    return tushare_token

class StockDataCollector:
    def __init__(self, token, cache_dir='cache', batch_size=50):
        # åˆå§‹åŒ–Tushare
        ts.set_token(token)
        self.pro = ts.pro_api()
        logger.info("Tushare API åˆå§‹åŒ–æˆåŠŸ")
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        self.cache_dir = cache_dir
        self.batch_size = batch_size
        os.makedirs(cache_dir, exist_ok=True)
        
    def _get_batch_cache_path(self, batch_index):
        """è·å–æ‰¹æ¬¡ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return os.path.join(self.cache_dir, f"batch_{batch_index}.json")
        
    def _save_batch_to_cache(self, batch_data, batch_index):
        """ä¿å­˜æ‰¹æ¬¡æ•°æ®åˆ°ç¼“å­˜"""
        if not batch_data:
            return
            
        cache_path = self._get_batch_cache_path(batch_index)
        try:
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(batch_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ä¿å­˜æ‰¹æ¬¡ç¼“å­˜å¤±è´¥ {cache_path}: {e}")
            
    def _load_batch_from_cache(self, batch_index):
        """ä»ç¼“å­˜åŠ è½½æ‰¹æ¬¡æ•°æ®"""
        cache_path = self._get_batch_cache_path(batch_index)
        if not os.path.exists(cache_path):
            return None
            
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"è¯»å–æ‰¹æ¬¡ç¼“å­˜å¤±è´¥ {cache_path}: {e}")
            return None

    def get_all_stocks(self):
        """è·å–æ‰€æœ‰Aè‚¡ä¸Šå¸‚å…¬å¸åˆ—è¡¨"""
        try:
            # ä»APIè·å–æ•°æ®
            stocks = self.pro.stock_basic(exchange='', list_status='L')
            return stocks[['ts_code', 'name', 'industry']]
        except Exception as e:
            logger.error(f"è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            return None

    def get_annual_data(self, stock_code, start_year, end_year):
        """è·å–å•ä¸ªè‚¡ç¥¨çš„å¹´åº¦æ•°æ®"""
        try:
            # æ„å»ºå¹´ä»½èŒƒå›´ï¼ˆæ’é™¤å½“å‰å¹´ä»½ï¼Œåªè·å–å®Œæ•´å¹´ä»½çš„æ•°æ®ï¼‰
            current_year = datetime.now().year
            actual_end_year = min(end_year, current_year - 1)  # ä¸åŒ…å«å½“å‰å¹´ä»½
            years = range(start_year, actual_end_year + 1)
            
            data = {
                'financial_indicators': [],
                'dividend': [],
                'pe': []
            }
            
            # è·å–æ¯å¹´çš„å¹´æŠ¥è´¢åŠ¡æŒ‡æ ‡
            for year in years:
                # è·å–å¹´æŠ¥è´¢åŠ¡æŒ‡æ ‡ï¼ˆä½¿ç”¨å¹´æœ«æ—¥æœŸï¼‰
                year_end = f"{year}1231"
                
                indicators = self.pro.fina_indicator(
                    ts_code=stock_code,
                    end_date=year_end,
                    period_type='Y',
                    fields='ts_code,end_date,roe,grossprofit_margin,netprofit_margin'
                )
                if indicators is not None and not indicators.empty:
                    # è¿‡æ»¤å‡ºè¯¥å¹´çš„å¹´æŠ¥æ•°æ®
                    year_indicators = indicators[indicators['end_date'].str.startswith(str(year))]
                    if not year_indicators.empty:
                        # åªå–æœ€æ–°çš„ä¸€æ¡å¹´æŠ¥æ•°æ®
                        latest_indicator = year_indicators.iloc[0:1]
                        data['financial_indicators'].extend(latest_indicator.to_dict('records'))
                
                # è·å–å¹´æœ«è‚¡æ¯ç‡ï¼ˆå°è¯•å¤šä¸ªæ—¥æœŸï¼‰
                dividend_found = False
                for month_day in ['1231', '1230', '1229', '1228']:  # å°è¯•å¹´æœ«å‡ å¤©
                    test_date = f"{year}{month_day}"
                    dividend = self.pro.daily_basic(
                        ts_code=stock_code,
                        trade_date=test_date,
                        fields='ts_code,trade_date,dv_ratio'
                    )
                    if dividend is not None and not dividend.empty:
                        data['dividend'].extend(dividend.to_dict('records'))
                        dividend_found = True
                        break
                    time.sleep(0.1)  # çŸ­æš‚å»¶æ—¶
                
                # è·å–å¹´æœ«PEï¼ˆå°è¯•å¤šä¸ªæ—¥æœŸï¼‰
                pe_found = False
                for month_day in ['1231', '1230', '1229', '1228']:  # å°è¯•å¹´æœ«å‡ å¤©
                    test_date = f"{year}{month_day}"
                    pe = self.pro.daily_basic(
                        ts_code=stock_code,
                        trade_date=test_date,
                        fields='ts_code,trade_date,pe'
                    )
                    if pe is not None and not pe.empty:
                        data['pe'].extend(pe.to_dict('records'))
                        pe_found = True
                        break
                    time.sleep(0.1)  # çŸ­æš‚å»¶æ—¶
                
                time.sleep(0.3)  # æ·»åŠ å»¶æ—¶é¿å…é¢‘ç‡é™åˆ¶
            
            return data
            
        except Exception as e:
            logger.error(f"è·å–å¹´åº¦æ•°æ®å¤±è´¥ {stock_code}: {e}")
            return None

    def process_batch(self, stocks_batch, start_year, end_year, use_cache=True):
        """å¤„ç†ä¸€æ‰¹è‚¡ç¥¨æ•°æ®"""
        batch_index = stocks_batch.index[0] // self.batch_size
        
        # å°è¯•ä»ç¼“å­˜åŠ è½½
        if use_cache:
            cached_data = self._load_batch_from_cache(batch_index)
            if cached_data is not None:
                return cached_data
        
        # è·å–æ–°æ•°æ®
        batch_data = {}
        for _, stock in stocks_batch.iterrows():
            stock_code = stock['ts_code']
            stock_data = self.get_annual_data(stock_code, start_year, end_year)
            if stock_data:
                batch_data[stock_code] = {
                    'name': stock['name'],
                    'industry': stock['industry'],
                    'data': stock_data
                }
        
        # ä¿å­˜åˆ°ç¼“å­˜
        if batch_data:
            self._save_batch_to_cache(batch_data, batch_index)
        
        return batch_data

class ExcelOptimizer:
    """Excelæ•°æ®ä¼˜åŒ–å™¨"""
    def __init__(self, df):
        self.df = df
        
    def create_summary_view(self):
        """åˆ›å»ºæ±‡æ€»è§†å›¾ - åªæ˜¾ç¤ºå…³é”®ä¿¡æ¯"""
        if self.df is None:
            return None
            
        # åŸºæœ¬ä¿¡æ¯åˆ—
        basic_cols = ['stock_code', 'stock_name', 'industry', 'need_analysis']
        
        # è®¡ç®—å„æŒ‡æ ‡çš„æœ€æ–°å€¼å’Œå¹³å‡å€¼
        summary_data = []
        
        for _, row in self.df.iterrows():
            summary_row = {}
            
            # åŸºæœ¬ä¿¡æ¯
            for col in basic_cols:
                if col in row:
                    summary_row[col] = row[col]
            
            # ROEæ±‡æ€»
            roe_cols = [col for col in self.df.columns if col.startswith('roe_')]
            roe_values = [row[col] for col in roe_cols if pd.notna(row[col])]
            if roe_values:
                summary_row['roe_æœ€æ–°'] = roe_values[-1]
                summary_row['roe_å¹³å‡'] = np.mean(roe_values)
                summary_row['roe_è¶‹åŠ¿'] = 'ä¸Šå‡' if len(roe_values) > 1 and roe_values[-1] > roe_values[0] else 'ä¸‹é™'
            
            # æ¯›åˆ©ç‡æ±‡æ€»
            gm_cols = [col for col in self.df.columns if col.startswith('gross_margin_')]
            gm_values = [row[col] for col in gm_cols if pd.notna(row[col])]
            if gm_values:
                summary_row['æ¯›åˆ©ç‡_æœ€æ–°'] = gm_values[-1]
                summary_row['æ¯›åˆ©ç‡_å¹³å‡'] = np.mean(gm_values)
            
            # å‡€åˆ©ç‡æ±‡æ€»
            nm_cols = [col for col in self.df.columns if col.startswith('net_margin_')]
            nm_values = [row[col] for col in nm_cols if pd.notna(row[col])]
            if nm_values:
                summary_row['å‡€åˆ©ç‡_æœ€æ–°'] = nm_values[-1]
                summary_row['å‡€åˆ©ç‡_å¹³å‡'] = np.mean(nm_values)
            
            # PEæ±‡æ€»
            pe_cols = [col for col in self.df.columns if col.startswith('pe_')]
            pe_values = [row[col] for col in pe_cols if pd.notna(row[col])]
            if pe_values:
                summary_row['PE_æœ€æ–°'] = pe_values[-1]
                summary_row['PE_å¹³å‡'] = np.mean(pe_values)
            
            # è‚¡æ¯ç‡æ±‡æ€»
            div_cols = [col for col in self.df.columns if col.startswith('dividend_')]
            div_values = [row[col] for col in div_cols if pd.notna(row[col])]
            if div_values:
                summary_row['è‚¡æ¯ç‡_æœ€æ–°'] = div_values[-1]
                summary_row['è‚¡æ¯ç‡_å¹³å‡'] = np.mean(div_values)
            
            # ç»¼åˆè¯„åˆ†ï¼ˆç®€å•è¯„åˆ†é€»è¾‘ï¼‰
            score = 0
            if 'roe_å¹³å‡' in summary_row and summary_row['roe_å¹³å‡'] > 15:
                score += 20
            if 'æ¯›åˆ©ç‡_å¹³å‡' in summary_row and summary_row['æ¯›åˆ©ç‡_å¹³å‡'] > 30:
                score += 20
            if 'å‡€åˆ©ç‡_å¹³å‡' in summary_row and summary_row['å‡€åˆ©ç‡_å¹³å‡'] > 10:
                score += 20
            if 'PE_å¹³å‡' in summary_row and 10 < summary_row['PE_å¹³å‡'] < 25:
                score += 20
            if 'è‚¡æ¯ç‡_å¹³å‡' in summary_row and summary_row['è‚¡æ¯ç‡_å¹³å‡'] > 2:
                score += 20
            
            summary_row['ç»¼åˆè¯„åˆ†'] = score
            summary_data.append(summary_row)
        
        return pd.DataFrame(summary_data)
    
    def create_sector_analysis(self):
        """åˆ›å»ºè¡Œä¸šåˆ†æè§†å›¾"""
        if self.df is None:
            return None
            
        # æŒ‰è¡Œä¸šåˆ†ç»„ç»Ÿè®¡
        sector_stats = []
        
        for industry in self.df['industry'].unique():
            if pd.isna(industry):
                continue
                
            industry_data = self.df[self.df['industry'] == industry]
            
            # è®¡ç®—è¡Œä¸šå¹³å‡æŒ‡æ ‡
            roe_cols = [col for col in self.df.columns if col.startswith('roe_')]
            pe_cols = [col for col in self.df.columns if col.startswith('pe_')]
            
            sector_row = {
                'è¡Œä¸š': industry,
                'å…¬å¸æ•°é‡': len(industry_data),
                'å¹³å‡ROE': industry_data[roe_cols].mean().mean(),
                'å¹³å‡PE': industry_data[pe_cols].mean().mean(),
                'é«˜ROEå…¬å¸æ•°': (industry_data[roe_cols].mean(axis=1) > 15).sum(),
                'éœ€è¦åˆ†ææ•°': (industry_data['need_analysis'] == True).sum()
            }
            sector_stats.append(sector_row)
        
        return pd.DataFrame(sector_stats).sort_values('å¹³å‡ROE', ascending=False)
    
    def create_filtered_views(self):
        """åˆ›å»ºç­›é€‰è§†å›¾"""
        if self.df is None:
            return {}
            
        views = {}
        
        # é«˜ROEè‚¡ç¥¨ï¼ˆROEå‡å€¼>15%ï¼‰
        roe_cols = [col for col in self.df.columns if col.startswith('roe_')]
        high_roe_mask = self.df[roe_cols].mean(axis=1) > 15
        views['é«˜ROEè‚¡ç¥¨'] = self.df[high_roe_mask][['stock_code', 'stock_name', 'industry'] + roe_cols]
        
        # ä½PEè‚¡ç¥¨ï¼ˆPEå‡å€¼<20ï¼‰
        pe_cols = [col for col in self.df.columns if col.startswith('pe_')]
        low_pe_mask = self.df[pe_cols].mean(axis=1) < 20
        views['ä½PEè‚¡ç¥¨'] = self.df[low_pe_mask][['stock_code', 'stock_name', 'industry'] + pe_cols]
        
        # é«˜è‚¡æ¯è‚¡ç¥¨ï¼ˆè‚¡æ¯ç‡å‡å€¼>3%ï¼‰
        div_cols = [col for col in self.df.columns if col.startswith('dividend_')]
        high_div_mask = self.df[div_cols].mean(axis=1) > 3
        views['é«˜è‚¡æ¯è‚¡ç¥¨'] = self.df[high_div_mask][['stock_code', 'stock_name', 'industry'] + div_cols]
        
        # ç¨³å®šç›ˆåˆ©è‚¡ç¥¨ï¼ˆå‡€åˆ©ç‡è¿ç»­æ­£å€¼ï¼‰
        nm_cols = [col for col in self.df.columns if col.startswith('net_margin_')]
        stable_profit_mask = (self.df[nm_cols] > 0).all(axis=1)
        views['ç¨³å®šç›ˆåˆ©è‚¡ç¥¨'] = self.df[stable_profit_mask][['stock_code', 'stock_name', 'industry'] + nm_cols]
        
        return views
    
    def _apply_styles(self, excel_file):
        """åº”ç”¨Excelæ ·å¼"""
        try:
            wb = load_workbook(excel_file)
            
            # ä¸ºæ¯ä¸ªå·¥ä½œè¡¨æ·»åŠ æ ·å¼
            for sheet_name in wb.sheetnames:
                ws = wb[sheet_name]
                
                # è®¾ç½®æ ‡é¢˜è¡Œæ ·å¼
                header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
                header_font = Font(color="FFFFFF", bold=True)
                
                for col in range(1, ws.max_column + 1):
                    cell = ws.cell(row=1, column=col)
                    cell.fill = header_fill
                    cell.font = header_font
                    cell.alignment = Alignment(horizontal="center")
                
                # è®¾ç½®æ•°æ®è¡Œæ ·å¼
                for row in range(2, ws.max_row + 1):
                    for col in range(1, ws.max_column + 1):
                        cell = ws.cell(row=row, column=col)
                        cell.alignment = Alignment(horizontal="center")
                        
                        # ä¸ºæ•°å€¼ç±»å‹çš„å•å…ƒæ ¼è®¾ç½®æ¡ä»¶æ ¼å¼
                        if isinstance(cell.value, (int, float)):
                            if cell.value is not None and cell.value < 0:
                                cell.font = Font(color="FF0000")  # è´Ÿå€¼æ˜¾ç¤ºçº¢è‰²
                            elif cell.value is not None and cell.value > 20:
                                cell.font = Font(color="008000")  # é«˜å€¼æ˜¾ç¤ºç»¿è‰²
                
                # è‡ªåŠ¨è°ƒæ•´åˆ—å®½
                for column in ws.columns:
                    max_length = 0
                    column_letter = column[0].column_letter
                    for cell in column:
                        try:
                            if len(str(cell.value)) > max_length:
                                max_length = len(str(cell.value))
                        except:
                            pass
                    adjusted_width = min(max_length + 2, 20)
                    ws.column_dimensions[column_letter].width = adjusted_width
            
            wb.save(excel_file)
            
        except Exception as e:
            logger.error(f"åº”ç”¨æ ·å¼å¤±è´¥: {e}")
    
    def generate_analysis_suggestions(self):
        """ç”Ÿæˆåˆ†æå»ºè®®"""
        if self.df is None:
            return None
            
        suggestions = []
        
        # 1. é«˜ä»·å€¼è‚¡ç¥¨æ¨è
        summary_df = self.create_summary_view()
        if summary_df is not None:
            high_score_stocks = summary_df[summary_df['ç»¼åˆè¯„åˆ†'] >= 80].sort_values('ç»¼åˆè¯„åˆ†', ascending=False)
            if not high_score_stocks.empty:
                suggestions.append("ğŸŒŸ é«˜ä»·å€¼è‚¡ç¥¨æ¨èï¼š")
                for _, stock in high_score_stocks.head(10).iterrows():
                    suggestions.append(f"  â€¢ {stock['stock_name']}({stock['stock_code']}) - è¯„åˆ†:{stock['ç»¼åˆè¯„åˆ†']}")
        
        # 2. è¡Œä¸šæœºä¼š
        sector_df = self.create_sector_analysis()
        if sector_df is not None:
            top_sectors = sector_df.head(5)
            suggestions.append("\nğŸ“ˆ ä¼˜åŠ¿è¡Œä¸šï¼š")
            for _, sector in top_sectors.iterrows():
                suggestions.append(f"  â€¢ {sector['è¡Œä¸š']} - å¹³å‡ROE:{sector['å¹³å‡ROE']:.2f}%")
        
        # 3. ç­›é€‰å»ºè®®
        filtered_views = self.create_filtered_views()
        suggestions.append("\nğŸ” ç­›é€‰å»ºè®®ï¼š")
        for view_name, view_df in filtered_views.items():
            suggestions.append(f"  â€¢ {view_name}: {len(view_df)}åªè‚¡ç¥¨")
        
        return '\n'.join(suggestions)
    
    def save_optimized_excel(self, output_file='stock_analysis_optimized.xlsx'):
        """ä¿å­˜ä¼˜åŒ–åçš„Excelæ–‡ä»¶"""
        if self.df is None:
            logger.error("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return False
            
        try:
            with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
                # 1. æ±‡æ€»è§†å›¾
                summary_df = self.create_summary_view()
                if summary_df is not None:
                    summary_df.to_excel(writer, sheet_name='æ±‡æ€»è§†å›¾', index=False)
                    logger.info(f"æ±‡æ€»è§†å›¾å·²åˆ›å»ºï¼ŒåŒ…å«{len(summary_df)}è¡Œæ•°æ®")
                
                # 2. è¡Œä¸šåˆ†æ
                sector_df = self.create_sector_analysis()
                if sector_df is not None:
                    sector_df.to_excel(writer, sheet_name='è¡Œä¸šåˆ†æ', index=False)
                    logger.info(f"è¡Œä¸šåˆ†æå·²åˆ›å»ºï¼ŒåŒ…å«{len(sector_df)}ä¸ªè¡Œä¸š")
                
                # 3. ç­›é€‰è§†å›¾
                filtered_views = self.create_filtered_views()
                for view_name, view_df in filtered_views.items():
                    if not view_df.empty:
                        view_df.to_excel(writer, sheet_name=view_name, index=False)
                        logger.info(f"{view_name}å·²åˆ›å»ºï¼ŒåŒ…å«{len(view_df)}åªè‚¡ç¥¨")
                
                # 4. åŸå§‹æ•°æ®ï¼ˆå¯é€‰ï¼‰
                self.df.to_excel(writer, sheet_name='åŸå§‹æ•°æ®', index=False)
                logger.info("åŸå§‹æ•°æ®å·²ä¿ç•™")
            
            # æ·»åŠ æ ·å¼
            self._apply_styles(output_file)
            logger.info(f"ä¼˜åŒ–åçš„Excelæ–‡ä»¶å·²ä¿å­˜: {output_file}")
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜Excelæ–‡ä»¶å¤±è´¥: {e}")
            return False

def setup_logger():
    """é…ç½®æ—¥å¿—"""
    log_path = "logs"
    os.makedirs(log_path, exist_ok=True)
    logger.add(
        os.path.join(log_path, "data_collection_{time}.log"),
        rotation="500 MB",
        encoding="utf-8"
    )

def process_stock_data(batch_data):
    """å¤„ç†è‚¡ç¥¨æ•°æ®ä¸ºæœ€ç»ˆæ ¼å¼"""
    results = []
    for stock_code, stock_info in batch_data.items():
        row = {
            'stock_code': stock_code,
            'stock_name': stock_info['name'],
            'industry': stock_info['industry'],
            'need_analysis': False
        }
        
        # å¤„ç†è´¢åŠ¡æŒ‡æ ‡
        for indicator in stock_info['data']['financial_indicators']:
            year = indicator['end_date'][:4]
            row[f'roe_{year}'] = indicator['roe']
            row[f'gross_margin_{year}'] = indicator['grossprofit_margin']
            row[f'net_margin_{year}'] = indicator['netprofit_margin']
        
        # å¤„ç†è‚¡æ¯ç‡
        for dividend in stock_info['data']['dividend']:
            year = dividend['trade_date'][:4]
            row[f'dividend_{year}'] = dividend['dv_ratio']
        
        # å¤„ç†PE
        for pe_data in stock_info['data']['pe']:
            year = pe_data['trade_date'][:4]
            row[f'pe_{year}'] = pe_data['pe']
        
        results.append(row)
    
    return results

def main():
    """ä¸»ç¨‹åºå…¥å£"""
    setup_logger()
    
    # å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(description='Aè‚¡åŸºæœ¬é¢æ•°æ®æ”¶é›†å·¥å…·')
    parser.add_argument('--limit', type=int, default=None, help='é™åˆ¶å¤„ç†çš„è‚¡ç¥¨æ•°é‡ï¼ˆæµ‹è¯•ç”¨ï¼‰')
    parser.add_argument('--batch-size', type=int, default=50, help='æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--no-cache', action='store_true', help='ä¸ä½¿ç”¨ç¼“å­˜ï¼Œé‡æ–°è·å–æ•°æ®')
    parser.add_argument('--start-year', type=int, default=2018, help='å¼€å§‹å¹´ä»½')
    parser.add_argument('--end-year', type=int, default=2025, help='ç»“æŸå¹´ä»½')
    parser.add_argument('--no-optimize', action='store_true', help='ä¸ç”Ÿæˆä¼˜åŒ–Excelè§†å›¾')
    
    args = parser.parse_args()
    
    # ä»é…ç½®æ–‡ä»¶è·å–token
    token = config.TUSHARE_TOKEN
    if not token:
        logger.error("è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®TUSHARE_TOKEN")
        return
    
    # åˆå§‹åŒ–æ”¶é›†å™¨
    collector = StockDataCollector(token, cache_dir='cache', batch_size=args.batch_size)
    
    try:
        logger.info(f"æ•°æ®æ”¶é›†æ—¶é—´èŒƒå›´ï¼š{args.start_year} è‡³ {args.end_year}")
        
        # è·å–è‚¡ç¥¨åˆ—è¡¨
        stocks = collector.get_all_stocks()
        if stocks is None:
            logger.error("è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥")
            return
        
        # é™åˆ¶è‚¡ç¥¨æ•°é‡ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰
        if args.limit:
            stocks = stocks.head(args.limit)
            logger.info(f"é™åˆ¶æ¨¡å¼ï¼šåªå¤„ç†å‰ {args.limit} åªè‚¡ç¥¨")
        
        logger.info(f"å…±è·å–åˆ° {len(stocks)} åªè‚¡ç¥¨")
        
        # è®¡ç®—æ‰¹æ¬¡æ•°é‡
        total_batches = (len(stocks) + args.batch_size - 1) // args.batch_size
        logger.info(f"å°†åˆ† {total_batches} ä¸ªæ‰¹æ¬¡å¤„ç†")
        
        all_results = []
        
        # æŒ‰æ‰¹æ¬¡å¤„ç†
        for i in tqdm(range(total_batches), desc="å¤„ç†æ‰¹æ¬¡"):
            start_idx = i * args.batch_size
            end_idx = min((i + 1) * args.batch_size, len(stocks))
            stocks_batch = stocks.iloc[start_idx:end_idx]
            
            # å¤„ç†å½“å‰æ‰¹æ¬¡
            batch_data = collector.process_batch(
                stocks_batch,
                args.start_year,
                args.end_year,
                use_cache=not args.no_cache
            )
            
            # å¤„ç†æ•°æ®å¹¶æ·»åŠ åˆ°ç»“æœ
            if batch_data:
                batch_results = process_stock_data(batch_data)
                all_results.extend(batch_results)
                logger.info(f"å®Œæˆç¬¬ {i+1}/{total_batches} æ‰¹æ¬¡å¤„ç†ï¼Œå½“å‰å·²å¤„ç† {len(all_results)} åªè‚¡ç¥¨")
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        if all_results:
            df = pd.DataFrame(all_results)
            output_file = 'stock_analysis_data.xlsx'
            df.to_excel(output_file, index=False)
            logger.info(f"åŸå§‹æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
            logger.info(f"å…±å¤„ç†äº† {len(all_results)} åªè‚¡ç¥¨ï¼Œ{len(df.columns)} åˆ—æ•°æ®")
            
            # è‡ªåŠ¨ç”Ÿæˆä¼˜åŒ–è§†å›¾
            if not args.no_optimize:
                logger.info("å¼€å§‹ç”Ÿæˆä¼˜åŒ–Excelè§†å›¾...")
                optimizer = ExcelOptimizer(df)
                
                if optimizer.save_optimized_excel():
                    logger.info("âœ… ä¼˜åŒ–Excelæ–‡ä»¶åˆ›å»ºæˆåŠŸ: stock_analysis_optimized.xlsx")
                    
                    # ç”Ÿæˆåˆ†æå»ºè®®
                    suggestions = optimizer.generate_analysis_suggestions()
                    if suggestions:
                        logger.info("ğŸ“‹ æŠ•èµ„åˆ†æå»ºè®®ï¼š")
                        print("\n" + suggestions)
                        
                        # ä¿å­˜å»ºè®®åˆ°æ–‡ä»¶
                        with open('analysis_suggestions.txt', 'w', encoding='utf-8') as f:
                            f.write(suggestions)
                        logger.info("ğŸ’¾ åˆ†æå»ºè®®å·²ä¿å­˜åˆ°: analysis_suggestions.txt")
                    
                    print(f"\nğŸ¯ æ•°æ®å¤„ç†å®Œæˆï¼ç”Ÿæˆäº†ä»¥ä¸‹æ–‡ä»¶ï¼š")
                    print(f"  ğŸ“„ {output_file} - åŸå§‹æ•°æ® ({len(df.columns)}åˆ—)")
                    print(f"  ğŸ“Š stock_analysis_optimized.xlsx - ä¼˜åŒ–è§†å›¾ (7ä¸ªå·¥ä½œè¡¨)")
                    print(f"  ğŸ“ analysis_suggestions.txt - æŠ•èµ„å»ºè®®")
                else:
                    logger.error("ä¼˜åŒ–Excelæ–‡ä»¶åˆ›å»ºå¤±è´¥")
            else:
                logger.info("å·²è·³è¿‡ä¼˜åŒ–Excelè§†å›¾ç”Ÿæˆï¼ˆä½¿ç”¨--no-optimizeå‚æ•°ï¼‰")
                
        else:
            logger.error("æ²¡æœ‰æ”¶é›†åˆ°ä»»ä½•æ•°æ®")
            
    except Exception as e:
        logger.error(f"ä¸»ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")

if __name__ == "__main__":
    main() 